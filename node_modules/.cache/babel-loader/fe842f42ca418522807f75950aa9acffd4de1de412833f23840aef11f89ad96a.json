{"ast":null,"code":"var _jsxFileName = \"/Users/yashagrawal/Documents/Northwestern/Fall2023/CS338/InterviewPrep/src/components/VoiceToText.jsx\",\n  _s = $RefreshSig$();\n// import React, { useState, useRef, useEffect } from 'react';\n// import './VoiceToText.css';\n\n// // FOR NOW - TRANSCRIPT IS THE GPT RESPONSE INTO AUDIO, USERSPEECH IS THE SPEECH FROM USER GOING INTO GPT\n\n// const VoiceToText = ({ transcript, setTranscript, setUserSpeech }) => {\n//     const [isListening, setIsListening] = useState(false);\n//     // const [audioFile, setAudioFile] = useState(null);\n//     // const [isButtonDisabled, setIsButtonDisabled] = useState(false);\n//     const [finalSpeech, setFinalSpeech] = useState(null);\n\n//     // useEffect(() => {\n//     //     if (transcript) {\n//     //       handleGenerateAudio();\n//     //     }\n//     // }, [transcript]);\n\n//     // useEffect(() => {\n//     //     if (isButtonDisabled) {\n//     //         const timeout = setTimeout(() => {\n//     //             setIsButtonDisabled(false);\n//     //         }, 2000);\n\n//     //         return () => clearTimeout(timeout);\n//     //     }\n//     // }, [isButtonDisabled]);\n\n//     useEffect(() => {\n//         console.log('is listening? ' + isListening);\n//         if(!isListening && finalSpeech != ''){\n//             setUserSpeech(finalSpeech); // puts non-null user speech into the userSpeech variable accessible to gpt\n//             setFinalSpeech('');\n//         }\n//     }, [isListening]);\n\n//     const recognitionRef = useRef(null); \n\n//     const handleStop = () => {\n//         setIsListening(false);\n//         // setIsButtonDisabled(true);\n//         if (recognitionRef.current) {\n//             recognitionRef.current.onresult = null;  \n//             recognitionRef.current.stop();\n//         }\n//     };\n\n//     const handleStart = () => {\n//         setIsListening(true);\n\n//         if (window.SpeechRecognition || window.webkitSpeechRecognition) {\n//             const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n//             if (!recognitionRef.current) {\n//                 recognitionRef.current = new SpeechRecognition();\n//                 recognitionRef.current.continuous = true;\n//                 recognitionRef.current.interimResults = true;\n//             }\n\n//             recognitionRef.current.onstart = () => {};\n\n//             // automatically on api maybe you can stop the break up when pause\n//             // tricks to async waiting for state to update\n//             var speech = '';\n//             recognitionRef.current.onresult = (event) => {\n//                 for (let i = 0; i < event.results.length; i++) {\n//                     if (event.results[i].isFinal) {\n//                         const recentSpeech = event.results[i][0].transcript;\n//                         if(!speech.includes(recentSpeech)){\n//                             speech += recentSpeech;\n//                         }\n//                         setFinalSpeech(speech);\n//                         // console.log(finalSpeech);\n//                         // console.log(speech);\n//                     }\n//                 }\n//             };\n\n//             recognitionRef.current.onerror = (event) => {\n//                 console.error(\"Error occurred in recognition:\", event.error);\n//             };\n\n//             recognitionRef.current.start();\n//         } else {\n//             alert(\"Your browser does not support the Web Speech API. Please try a different browser.\");\n//         }\n\n//     };\n\n//     // const sendTranscriptToBackend = async (text) => {\n//     //     const backendUrl = 'http://127.0.0.1:5000/run_script';\n\n//     //     try {\n//     //         const response = await fetch(backendUrl, {\n//     //             method: 'POST',\n//     //             headers: {\n//     //                 'Content-Type': 'application/json'\n//     //             },\n//     //             body: JSON.stringify({ text: text })\n//     //         });\n\n//     //         if (!response.ok) {\n//     //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n//     //         }\n\n//     //         const responseData = await response.json();\n//     //         console.log(responseData);\n//     //         setAudioFile(responseData.audio_file);\n\n//     //     } catch (error) {\n//     //         console.error('Error sending transcript to backend:', error);\n//     //     }\n//     // };\n\n//     // const handleGenerateAudio = async () => {\n//     //     sendTranscriptToBackend(transcript);\n//     //    // handleStop();\n//     // };\n//     // const sendTranscriptToBackend = async (text) => {\n//     //     const apiUrl = \"https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM\";\n//     //     const payload = {\n//     //         \"text\": text,\n//     //         \"model_id\": \"eleven_monolingual_v1\",\n//     //         \"voice_settings\": {\n//     //             \"stability\": 0,\n//     //             \"similarity_boost\": 0,\n//     //             \"style\": 0,\n//     //             \"use_speaker_boost\": true\n//     //         }\n//     //     };\n\n//     //     const headers = {\n//     //         \"Content-Type\": \"application/json\",\n//     //         \"xi-api-key\": \"321547ec48256661cb0b640353bde72c\"\n//     //     };\n\n//     //     try {\n//     //         const response = await fetch(apiUrl, {\n//     //             method: 'POST',\n//     //             headers: headers,\n//     //             body: JSON.stringify(payload)\n//     //         });\n\n//     //         if (!response.ok) {\n//     //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n//     //         }\n\n//     //         const audioBlob = await response.blob();\n//     //         const audioUrl = URL.createObjectURL(audioBlob);\n//     //         setAudioFile(audioUrl);\n//     //         console.log('Response:', response);\n//     //         console.log('Audio Blob:', audioBlob);\n//     //         console.log('Audio URL:', audioUrl);\n\n//     //     } catch (error) {\n//     //         console.error('Error sending transcript to Eleven Labs:', error);\n//     //     }\n\n//     //     handleStop();\n//     // };\n\n//     return (\n//         <div>\n//             <button\n//                 id=\"generate-audio-button\"\n//                 // class={`voice-to-text-button ${isButtonDisabled ? 'disabled' : ''} ${isListening ? 'green' : 'red'}`}\n//                 class={`voice-to-text-button ${isListening ? 'green' : 'red'}`}\n//                 onClick={isListening ? handleStop : handleStart}\n//                 // disabled={isButtonDisabled}\n//             >\n//                 <div class=\"mic-icon\"></div>\n//             </button>\n\n//             {/* {audioFile && (\n//                 <div>\n//                     <h3>Your Generated Audio File:</h3>\n//                     <audio controls src={audioFile}>\n//                         Your browser does not support the audio tag.\n//                     </audio>\n//                 </div>\n//             )} */}\n\n//             <div className=\"transcript-display\">\n//                 <h2>Transcript:</h2>\n//                 <p>{transcript}</p>\n//             </div>\n//         </div>\n//     );\n\n// };\n\n// export default VoiceToText;\n\nimport React, { useState, useRef, useEffect } from 'react';\nimport './VoiceToText.css';\n\n// FOR NOW - TRANSCRIPT IS THE GPT RESPONSE INTO AUDIO, USERSPEECH IS THE SPEECH FROM USER GOING INTO GPT\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst VoiceToText = ({\n  transcript,\n  setTranscript,\n  setUserSpeech\n}) => {\n  _s();\n  const [isListening, setIsListening] = useState(false);\n  const [audioFile, setAudioFile] = useState(null);\n  const [isButtonDisabled, setIsButtonDisabled] = useState(false);\n  const [finalSpeech, setFinalSpeech] = useState(null);\n  useEffect(() => {\n    if (transcript) {\n      handleGenerateAudio();\n    }\n  }, [transcript]);\n  useEffect(() => {\n    if (isButtonDisabled) {\n      const timeout = setTimeout(() => {\n        setIsButtonDisabled(false);\n      }, 2000);\n      return () => clearTimeout(timeout);\n    }\n  }, [isButtonDisabled]);\n  useEffect(() => {\n    console.log('is listening? ' + isListening);\n    if (!isListening && finalSpeech != '') {\n      setUserSpeech(finalSpeech); // puts non-null user speech into the userSpeech variable accessible to gpt\n      setFinalSpeech('');\n    }\n  }, [isListening]);\n  const recognitionRef = useRef(null);\n  const handleStop = () => {\n    setIsListening(false);\n    setIsButtonDisabled(true);\n    if (recognitionRef.current) {\n      recognitionRef.current.onresult = null;\n      recognitionRef.current.stop();\n    }\n  };\n  const handleStart = () => {\n    let finalSpeech = '';\n    if (!isListening) {\n      if (window.SpeechRecognition || window.webkitSpeechRecognition) {\n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        if (!recognitionRef.current) {\n          recognitionRef.current = new SpeechRecognition();\n          recognitionRef.current.continuous = true;\n          recognitionRef.current.interimResults = true;\n        }\n        setIsListening(true);\n        recognitionRef.current.onstart = () => {};\n\n        // automatically on api maybe you can stop the break up when pause\n        // tricks to async waiting for state to update\n        var speech = '';\n        recognitionRef.current.onresult = event => {\n          for (let i = 0; i < event.results.length; i++) {\n            if (event.results[i].isFinal) {\n              const recentSpeech = event.results[i][0].transcript;\n              if (!speech.includes(recentSpeech)) {\n                speech += recentSpeech;\n              }\n              setFinalSpeech(speech);\n              // console.log(finalSpeech);\n              // console.log(speech);\n            }\n          }\n        };\n\n        recognitionRef.current.onerror = event => {\n          console.error(\"Error occurred in recognition:\", event.error);\n        };\n        recognitionRef.current.start();\n      } else {\n        alert(\"Your browser does not support the Web Speech API. Please try a different browser.\");\n      }\n    }\n  };\n  const handleGenerateAudio = async () => {\n    sendTranscriptToBackend(transcript);\n    // handleStop();\n  };\n\n  const sendTranscriptToBackend = async text => {\n    const backendUrl = 'http://127.0.0.1:5000/run_script';\n    try {\n      const response = await fetch(backendUrl, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          text: text\n        })\n      });\n      if (!response.ok) {\n        throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n      }\n      const responseData = await response.json();\n      console.log(responseData);\n      setAudioFile(responseData.audio_file);\n    } catch (error) {\n      console.error('Error sending transcript to backend:', error);\n    }\n  };\n\n  // const sendTranscriptToBackend = async (text) => {\n  //     const apiUrl = \"https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM\";\n  //     const payload = {\n  //         \"text\": text,\n  //         \"model_id\": \"eleven_monolingual_v1\",\n  //         \"voice_settings\": {\n  //             \"stability\": 0,\n  //             \"similarity_boost\": 0,\n  //             \"style\": 0,\n  //             \"use_speaker_boost\": true\n  //         }\n  //     };\n\n  //     const headers = {\n  //         \"Content-Type\": \"application/json\",\n  //         \"xi-api-key\": \"321547ec48256661cb0b640353bde72c\"\n  //     };\n\n  //     try {\n  //         const response = await fetch(apiUrl, {\n  //             method: 'POST',\n  //             headers: headers,\n  //             body: JSON.stringify(payload)\n  //         });\n\n  //         if (!response.ok) {\n  //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n  //         }\n\n  //         const audioBlob = await response.blob();\n  //         const audioUrl = URL.createObjectURL(audioBlob);\n  //         setAudioFile(audioUrl);\n  //         console.log('Response:', response);\n  //         console.log('Audio Blob:', audioBlob);\n  //         console.log('Audio URL:', audioUrl);\n\n  //     } catch (error) {\n  //         console.error('Error sending transcript to Eleven Labs:', error);\n  //     }\n\n  //     handleStop();\n  // };\n\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [/*#__PURE__*/_jsxDEV(\"button\", {\n      id: \"generate-audio-button\",\n      class: `voice-to-text-button ${isButtonDisabled ? 'disabled' : ''} ${isListening ? 'green' : 'red'}`,\n      onClick: isListening ? handleStop : handleStart,\n      disabled: isButtonDisabled,\n      children: /*#__PURE__*/_jsxDEV(\"div\", {\n        class: \"mic-icon\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 363,\n        columnNumber: 17\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 358,\n      columnNumber: 13\n    }, this), audioFile && /*#__PURE__*/_jsxDEV(\"div\", {\n      children: [/*#__PURE__*/_jsxDEV(\"h3\", {\n        children: \"Your Generated Audio File:\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 368,\n        columnNumber: 21\n      }, this), /*#__PURE__*/_jsxDEV(\"audio\", {\n        controls: true,\n        src: audioFile,\n        children: \"Your browser does not support the audio tag.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 369,\n        columnNumber: 21\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 367,\n      columnNumber: 17\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"transcript-display\",\n      children: [/*#__PURE__*/_jsxDEV(\"h2\", {\n        children: \"Transcript:\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 376,\n        columnNumber: 17\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: transcript\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 377,\n        columnNumber: 17\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 375,\n      columnNumber: 13\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 357,\n    columnNumber: 9\n  }, this);\n};\n_s(VoiceToText, \"Ez7m0L1Iu8V3hLgEkQvzv8jpJB4=\");\n_c = VoiceToText;\nexport default VoiceToText;\nvar _c;\n$RefreshReg$(_c, \"VoiceToText\");","map":{"version":3,"names":["React","useState","useRef","useEffect","jsxDEV","_jsxDEV","VoiceToText","transcript","setTranscript","setUserSpeech","_s","isListening","setIsListening","audioFile","setAudioFile","isButtonDisabled","setIsButtonDisabled","finalSpeech","setFinalSpeech","handleGenerateAudio","timeout","setTimeout","clearTimeout","console","log","recognitionRef","handleStop","current","onresult","stop","handleStart","window","SpeechRecognition","webkitSpeechRecognition","continuous","interimResults","onstart","speech","event","i","results","length","isFinal","recentSpeech","includes","onerror","error","start","alert","sendTranscriptToBackend","text","backendUrl","response","fetch","method","headers","body","JSON","stringify","ok","Error","status","statusText","responseData","json","audio_file","children","id","class","onClick","disabled","fileName","_jsxFileName","lineNumber","columnNumber","controls","src","className","_c","$RefreshReg$"],"sources":["/Users/yashagrawal/Documents/Northwestern/Fall2023/CS338/InterviewPrep/src/components/VoiceToText.jsx"],"sourcesContent":["// import React, { useState, useRef, useEffect } from 'react';\n// import './VoiceToText.css';\n\n// // FOR NOW - TRANSCRIPT IS THE GPT RESPONSE INTO AUDIO, USERSPEECH IS THE SPEECH FROM USER GOING INTO GPT\n\n// const VoiceToText = ({ transcript, setTranscript, setUserSpeech }) => {\n//     const [isListening, setIsListening] = useState(false);\n//     // const [audioFile, setAudioFile] = useState(null);\n//     // const [isButtonDisabled, setIsButtonDisabled] = useState(false);\n//     const [finalSpeech, setFinalSpeech] = useState(null);\n\n//     // useEffect(() => {\n//     //     if (transcript) {\n//     //       handleGenerateAudio();\n//     //     }\n//     // }, [transcript]);\n\n//     // useEffect(() => {\n//     //     if (isButtonDisabled) {\n//     //         const timeout = setTimeout(() => {\n//     //             setIsButtonDisabled(false);\n//     //         }, 2000);\n\n//     //         return () => clearTimeout(timeout);\n//     //     }\n//     // }, [isButtonDisabled]);\n\n//     useEffect(() => {\n//         console.log('is listening? ' + isListening);\n//         if(!isListening && finalSpeech != ''){\n//             setUserSpeech(finalSpeech); // puts non-null user speech into the userSpeech variable accessible to gpt\n//             setFinalSpeech('');\n//         }\n//     }, [isListening]);\n\n//     const recognitionRef = useRef(null); \n\n//     const handleStop = () => {\n//         setIsListening(false);\n//         // setIsButtonDisabled(true);\n//         if (recognitionRef.current) {\n//             recognitionRef.current.onresult = null;  \n//             recognitionRef.current.stop();\n//         }\n//     };\n\n//     const handleStart = () => {\n//         setIsListening(true);\n\n//         if (window.SpeechRecognition || window.webkitSpeechRecognition) {\n//             const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n//             if (!recognitionRef.current) {\n//                 recognitionRef.current = new SpeechRecognition();\n//                 recognitionRef.current.continuous = true;\n//                 recognitionRef.current.interimResults = true;\n//             }\n\n//             recognitionRef.current.onstart = () => {};\n\n//             // automatically on api maybe you can stop the break up when pause\n//             // tricks to async waiting for state to update\n//             var speech = '';\n//             recognitionRef.current.onresult = (event) => {\n//                 for (let i = 0; i < event.results.length; i++) {\n//                     if (event.results[i].isFinal) {\n//                         const recentSpeech = event.results[i][0].transcript;\n//                         if(!speech.includes(recentSpeech)){\n//                             speech += recentSpeech;\n//                         }\n//                         setFinalSpeech(speech);\n//                         // console.log(finalSpeech);\n//                         // console.log(speech);\n//                     }\n//                 }\n//             };\n\n//             recognitionRef.current.onerror = (event) => {\n//                 console.error(\"Error occurred in recognition:\", event.error);\n//             };\n\n//             recognitionRef.current.start();\n//         } else {\n//             alert(\"Your browser does not support the Web Speech API. Please try a different browser.\");\n//         }\n\n//     };\n\n//     // const sendTranscriptToBackend = async (text) => {\n//     //     const backendUrl = 'http://127.0.0.1:5000/run_script';\n\n//     //     try {\n//     //         const response = await fetch(backendUrl, {\n//     //             method: 'POST',\n//     //             headers: {\n//     //                 'Content-Type': 'application/json'\n//     //             },\n//     //             body: JSON.stringify({ text: text })\n//     //         });\n\n//     //         if (!response.ok) {\n//     //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n//     //         }\n\n//     //         const responseData = await response.json();\n//     //         console.log(responseData);\n//     //         setAudioFile(responseData.audio_file);\n\n//     //     } catch (error) {\n//     //         console.error('Error sending transcript to backend:', error);\n//     //     }\n//     // };\n    \n       \n\n//     // const handleGenerateAudio = async () => {\n//     //     sendTranscriptToBackend(transcript);\n//     //    // handleStop();\n//     // };\n//     // const sendTranscriptToBackend = async (text) => {\n//     //     const apiUrl = \"https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM\";\n//     //     const payload = {\n//     //         \"text\": text,\n//     //         \"model_id\": \"eleven_monolingual_v1\",\n//     //         \"voice_settings\": {\n//     //             \"stability\": 0,\n//     //             \"similarity_boost\": 0,\n//     //             \"style\": 0,\n//     //             \"use_speaker_boost\": true\n//     //         }\n//     //     };\n\n//     //     const headers = {\n//     //         \"Content-Type\": \"application/json\",\n//     //         \"xi-api-key\": \"321547ec48256661cb0b640353bde72c\"\n//     //     };\n\n//     //     try {\n//     //         const response = await fetch(apiUrl, {\n//     //             method: 'POST',\n//     //             headers: headers,\n//     //             body: JSON.stringify(payload)\n//     //         });\n\n//     //         if (!response.ok) {\n//     //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n//     //         }\n\n//     //         const audioBlob = await response.blob();\n//     //         const audioUrl = URL.createObjectURL(audioBlob);\n//     //         setAudioFile(audioUrl);\n//     //         console.log('Response:', response);\n//     //         console.log('Audio Blob:', audioBlob);\n//     //         console.log('Audio URL:', audioUrl);\n\n//     //     } catch (error) {\n//     //         console.error('Error sending transcript to Eleven Labs:', error);\n//     //     }\n\n//     //     handleStop();\n//     // };\n\n//     return (\n//         <div>\n//             <button\n//                 id=\"generate-audio-button\"\n//                 // class={`voice-to-text-button ${isButtonDisabled ? 'disabled' : ''} ${isListening ? 'green' : 'red'}`}\n//                 class={`voice-to-text-button ${isListening ? 'green' : 'red'}`}\n//                 onClick={isListening ? handleStop : handleStart}\n//                 // disabled={isButtonDisabled}\n//             >\n//                 <div class=\"mic-icon\"></div>\n//             </button>\n    \n//             {/* {audioFile && (\n//                 <div>\n//                     <h3>Your Generated Audio File:</h3>\n//                     <audio controls src={audioFile}>\n//                         Your browser does not support the audio tag.\n//                     </audio>\n//                 </div>\n//             )} */}\n\n//             <div className=\"transcript-display\">\n//                 <h2>Transcript:</h2>\n//                 <p>{transcript}</p>\n//             </div>\n//         </div>\n//     );\n    \n// };\n\n// export default VoiceToText;\n\nimport React, { useState, useRef, useEffect } from 'react';\nimport './VoiceToText.css';\n\n// FOR NOW - TRANSCRIPT IS THE GPT RESPONSE INTO AUDIO, USERSPEECH IS THE SPEECH FROM USER GOING INTO GPT\n\nconst VoiceToText = ({ transcript, setTranscript, setUserSpeech }) => {\n    const [isListening, setIsListening] = useState(false);\n    const [audioFile, setAudioFile] = useState(null);\n    const [isButtonDisabled, setIsButtonDisabled] = useState(false);\n    const [finalSpeech, setFinalSpeech] = useState(null);\n\n    useEffect(() => {\n        if (transcript) {\n          handleGenerateAudio();\n        }\n    }, [transcript]);\n\n    useEffect(() => {\n    if (isButtonDisabled) {\n      const timeout = setTimeout(() => {\n        setIsButtonDisabled(false);\n      }, 2000);\n\n      return () => clearTimeout(timeout);\n    }\n  }, [isButtonDisabled]);\n\n  useEffect(() => {\n    console.log('is listening? ' + isListening);\n    if(!isListening && finalSpeech != ''){\n        setUserSpeech(finalSpeech); // puts non-null user speech into the userSpeech variable accessible to gpt\n        setFinalSpeech('');\n    }\n  }, [isListening]);\n\n    const recognitionRef = useRef(null); \n\n    const handleStop = () => {\n        setIsListening(false);\n        setIsButtonDisabled(true);\n        if (recognitionRef.current) {\n            recognitionRef.current.onresult = null;  \n            recognitionRef.current.stop();\n        }\n    };\n\n    const handleStart = () => {\n        let finalSpeech = '';\n        if(!isListening){\n            if (window.SpeechRecognition || window.webkitSpeechRecognition) {\n                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n                if (!recognitionRef.current) {\n                    recognitionRef.current = new SpeechRecognition();\n                    recognitionRef.current.continuous = true;\n                    recognitionRef.current.interimResults = true;\n                }\n\n                setIsListening(true);\n                recognitionRef.current.onstart = () => {};\n\n                // automatically on api maybe you can stop the break up when pause\n                // tricks to async waiting for state to update\n                var speech = '';\n                recognitionRef.current.onresult = (event) => {\n                    for (let i = 0; i < event.results.length; i++) {\n                        if (event.results[i].isFinal) {\n                            const recentSpeech = event.results[i][0].transcript;\n                            if(!speech.includes(recentSpeech)){\n                                speech += recentSpeech;\n                            }\n                            setFinalSpeech(speech);\n                            // console.log(finalSpeech);\n                            // console.log(speech);\n                        }\n                    }\n                };\n\n                recognitionRef.current.onerror = (event) => {\n                    console.error(\"Error occurred in recognition:\", event.error);\n                };\n\n                recognitionRef.current.start();\n            } else {\n                alert(\"Your browser does not support the Web Speech API. Please try a different browser.\");\n            }\n        }\n    };\n    \n\n    const handleGenerateAudio = async () => {\n        sendTranscriptToBackend(transcript);\n       // handleStop();\n    };\n\n    const sendTranscriptToBackend = async (text) => {\n        const backendUrl = 'http://127.0.0.1:5000/run_script';\n\n        try {\n            const response = await fetch(backendUrl, {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json'\n                },\n                body: JSON.stringify({ text: text })\n            });\n\n            if (!response.ok) {\n                throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n            }\n\n            const responseData = await response.json();\n            console.log(responseData);\n            setAudioFile(responseData.audio_file);\n\n        } catch (error) {\n            console.error('Error sending transcript to backend:', error);\n        }\n    };\n    \n    // const sendTranscriptToBackend = async (text) => {\n    //     const apiUrl = \"https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM\";\n    //     const payload = {\n    //         \"text\": text,\n    //         \"model_id\": \"eleven_monolingual_v1\",\n    //         \"voice_settings\": {\n    //             \"stability\": 0,\n    //             \"similarity_boost\": 0,\n    //             \"style\": 0,\n    //             \"use_speaker_boost\": true\n    //         }\n    //     };\n\n    //     const headers = {\n    //         \"Content-Type\": \"application/json\",\n    //         \"xi-api-key\": \"321547ec48256661cb0b640353bde72c\"\n    //     };\n\n    //     try {\n    //         const response = await fetch(apiUrl, {\n    //             method: 'POST',\n    //             headers: headers,\n    //             body: JSON.stringify(payload)\n    //         });\n\n    //         if (!response.ok) {\n    //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n    //         }\n\n    //         const audioBlob = await response.blob();\n    //         const audioUrl = URL.createObjectURL(audioBlob);\n    //         setAudioFile(audioUrl);\n    //         console.log('Response:', response);\n    //         console.log('Audio Blob:', audioBlob);\n    //         console.log('Audio URL:', audioUrl);\n\n    //     } catch (error) {\n    //         console.error('Error sending transcript to Eleven Labs:', error);\n    //     }\n\n    //     handleStop();\n    // };\n\n    return (\n        <div>\n            <button\n                id=\"generate-audio-button\"\n                class={`voice-to-text-button ${isButtonDisabled ? 'disabled' : ''} ${isListening ? 'green' : 'red'}`}\n                onClick={isListening ? handleStop : handleStart}\n                disabled={isButtonDisabled}>\n                <div class=\"mic-icon\"></div>\n            </button>\n    \n            {audioFile && (\n                <div>\n                    <h3>Your Generated Audio File:</h3>\n                    <audio controls src={audioFile}>\n                        Your browser does not support the audio tag.\n                    </audio>\n                </div>\n            )}\n\n            <div className=\"transcript-display\">\n                <h2>Transcript:</h2>\n                <p>{transcript}</p>\n            </div>\n        </div>\n    );\n    \n};\n\nexport default VoiceToText;"],"mappings":";;AAAA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA,OAAOA,KAAK,IAAIC,QAAQ,EAAEC,MAAM,EAAEC,SAAS,QAAQ,OAAO;AAC1D,OAAO,mBAAmB;;AAE1B;AAAA,SAAAC,MAAA,IAAAC,OAAA;AAEA,MAAMC,WAAW,GAAGA,CAAC;EAAEC,UAAU;EAAEC,aAAa;EAAEC;AAAc,CAAC,KAAK;EAAAC,EAAA;EAClE,MAAM,CAACC,WAAW,EAAEC,cAAc,CAAC,GAAGX,QAAQ,CAAC,KAAK,CAAC;EACrD,MAAM,CAACY,SAAS,EAAEC,YAAY,CAAC,GAAGb,QAAQ,CAAC,IAAI,CAAC;EAChD,MAAM,CAACc,gBAAgB,EAAEC,mBAAmB,CAAC,GAAGf,QAAQ,CAAC,KAAK,CAAC;EAC/D,MAAM,CAACgB,WAAW,EAAEC,cAAc,CAAC,GAAGjB,QAAQ,CAAC,IAAI,CAAC;EAEpDE,SAAS,CAAC,MAAM;IACZ,IAAII,UAAU,EAAE;MACdY,mBAAmB,CAAC,CAAC;IACvB;EACJ,CAAC,EAAE,CAACZ,UAAU,CAAC,CAAC;EAEhBJ,SAAS,CAAC,MAAM;IAChB,IAAIY,gBAAgB,EAAE;MACpB,MAAMK,OAAO,GAAGC,UAAU,CAAC,MAAM;QAC/BL,mBAAmB,CAAC,KAAK,CAAC;MAC5B,CAAC,EAAE,IAAI,CAAC;MAER,OAAO,MAAMM,YAAY,CAACF,OAAO,CAAC;IACpC;EACF,CAAC,EAAE,CAACL,gBAAgB,CAAC,CAAC;EAEtBZ,SAAS,CAAC,MAAM;IACdoB,OAAO,CAACC,GAAG,CAAC,gBAAgB,GAAGb,WAAW,CAAC;IAC3C,IAAG,CAACA,WAAW,IAAIM,WAAW,IAAI,EAAE,EAAC;MACjCR,aAAa,CAACQ,WAAW,CAAC,CAAC,CAAC;MAC5BC,cAAc,CAAC,EAAE,CAAC;IACtB;EACF,CAAC,EAAE,CAACP,WAAW,CAAC,CAAC;EAEf,MAAMc,cAAc,GAAGvB,MAAM,CAAC,IAAI,CAAC;EAEnC,MAAMwB,UAAU,GAAGA,CAAA,KAAM;IACrBd,cAAc,CAAC,KAAK,CAAC;IACrBI,mBAAmB,CAAC,IAAI,CAAC;IACzB,IAAIS,cAAc,CAACE,OAAO,EAAE;MACxBF,cAAc,CAACE,OAAO,CAACC,QAAQ,GAAG,IAAI;MACtCH,cAAc,CAACE,OAAO,CAACE,IAAI,CAAC,CAAC;IACjC;EACJ,CAAC;EAED,MAAMC,WAAW,GAAGA,CAAA,KAAM;IACtB,IAAIb,WAAW,GAAG,EAAE;IACpB,IAAG,CAACN,WAAW,EAAC;MACZ,IAAIoB,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB,EAAE;QAC5D,MAAMD,iBAAiB,GAAGD,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB;QACpF,IAAI,CAACR,cAAc,CAACE,OAAO,EAAE;UACzBF,cAAc,CAACE,OAAO,GAAG,IAAIK,iBAAiB,CAAC,CAAC;UAChDP,cAAc,CAACE,OAAO,CAACO,UAAU,GAAG,IAAI;UACxCT,cAAc,CAACE,OAAO,CAACQ,cAAc,GAAG,IAAI;QAChD;QAEAvB,cAAc,CAAC,IAAI,CAAC;QACpBa,cAAc,CAACE,OAAO,CAACS,OAAO,GAAG,MAAM,CAAC,CAAC;;QAEzC;QACA;QACA,IAAIC,MAAM,GAAG,EAAE;QACfZ,cAAc,CAACE,OAAO,CAACC,QAAQ,GAAIU,KAAK,IAAK;UACzC,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGD,KAAK,CAACE,OAAO,CAACC,MAAM,EAAEF,CAAC,EAAE,EAAE;YAC3C,IAAID,KAAK,CAACE,OAAO,CAACD,CAAC,CAAC,CAACG,OAAO,EAAE;cAC1B,MAAMC,YAAY,GAAGL,KAAK,CAACE,OAAO,CAACD,CAAC,CAAC,CAAC,CAAC,CAAC,CAAChC,UAAU;cACnD,IAAG,CAAC8B,MAAM,CAACO,QAAQ,CAACD,YAAY,CAAC,EAAC;gBAC9BN,MAAM,IAAIM,YAAY;cAC1B;cACAzB,cAAc,CAACmB,MAAM,CAAC;cACtB;cACA;YACJ;UACJ;QACJ,CAAC;;QAEDZ,cAAc,CAACE,OAAO,CAACkB,OAAO,GAAIP,KAAK,IAAK;UACxCf,OAAO,CAACuB,KAAK,CAAC,gCAAgC,EAAER,KAAK,CAACQ,KAAK,CAAC;QAChE,CAAC;QAEDrB,cAAc,CAACE,OAAO,CAACoB,KAAK,CAAC,CAAC;MAClC,CAAC,MAAM;QACHC,KAAK,CAAC,mFAAmF,CAAC;MAC9F;IACJ;EACJ,CAAC;EAGD,MAAM7B,mBAAmB,GAAG,MAAAA,CAAA,KAAY;IACpC8B,uBAAuB,CAAC1C,UAAU,CAAC;IACpC;EACH,CAAC;;EAED,MAAM0C,uBAAuB,GAAG,MAAOC,IAAI,IAAK;IAC5C,MAAMC,UAAU,GAAG,kCAAkC;IAErD,IAAI;MACA,MAAMC,QAAQ,GAAG,MAAMC,KAAK,CAACF,UAAU,EAAE;QACrCG,MAAM,EAAE,MAAM;QACdC,OAAO,EAAE;UACL,cAAc,EAAE;QACpB,CAAC;QACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UAAER,IAAI,EAAEA;QAAK,CAAC;MACvC,CAAC,CAAC;MAEF,IAAI,CAACE,QAAQ,CAACO,EAAE,EAAE;QACd,MAAM,IAAIC,KAAK,CAAE,uBAAsBR,QAAQ,CAACS,MAAO,IAAGT,QAAQ,CAACU,UAAW,EAAC,CAAC;MACpF;MAEA,MAAMC,YAAY,GAAG,MAAMX,QAAQ,CAACY,IAAI,CAAC,CAAC;MAC1CzC,OAAO,CAACC,GAAG,CAACuC,YAAY,CAAC;MACzBjD,YAAY,CAACiD,YAAY,CAACE,UAAU,CAAC;IAEzC,CAAC,CAAC,OAAOnB,KAAK,EAAE;MACZvB,OAAO,CAACuB,KAAK,CAAC,sCAAsC,EAAEA,KAAK,CAAC;IAChE;EACJ,CAAC;;EAED;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;;EAEA,oBACIzC,OAAA;IAAA6D,QAAA,gBACI7D,OAAA;MACI8D,EAAE,EAAC,uBAAuB;MAC1BC,KAAK,EAAG,wBAAuBrD,gBAAgB,GAAG,UAAU,GAAG,EAAG,IAAGJ,WAAW,GAAG,OAAO,GAAG,KAAM,EAAE;MACrG0D,OAAO,EAAE1D,WAAW,GAAGe,UAAU,GAAGI,WAAY;MAChDwC,QAAQ,EAAEvD,gBAAiB;MAAAmD,QAAA,eAC3B7D,OAAA;QAAK+D,KAAK,EAAC;MAAU;QAAAG,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAM;IAAC;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACxB,CAAC,EAER7D,SAAS,iBACNR,OAAA;MAAA6D,QAAA,gBACI7D,OAAA;QAAA6D,QAAA,EAAI;MAA0B;QAAAK,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC,eACnCrE,OAAA;QAAOsE,QAAQ;QAACC,GAAG,EAAE/D,SAAU;QAAAqD,QAAA,EAAC;MAEhC;QAAAK,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAO,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACP,CACR,eAEDrE,OAAA;MAAKwE,SAAS,EAAC,oBAAoB;MAAAX,QAAA,gBAC/B7D,OAAA;QAAA6D,QAAA,EAAI;MAAW;QAAAK,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC,eACpBrE,OAAA;QAAA6D,QAAA,EAAI3D;MAAU;QAAAgE,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAClB,CAAC;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACL,CAAC;AAGd,CAAC;AAAChE,EAAA,CAvLIJ,WAAW;AAAAwE,EAAA,GAAXxE,WAAW;AAyLjB,eAAeA,WAAW;AAAC,IAAAwE,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}