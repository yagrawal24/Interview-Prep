{"ast":null,"code":"var _jsxFileName = \"/Users/yashagrawal/Documents/Northwestern/Fall2023/CS338/InterviewPrep/src/components/VoiceToText.jsx\",\n  _s = $RefreshSig$();\nimport React, { useState, useRef, useEffect } from 'react';\nimport './VoiceToText.css';\n\n// FOR NOW - TRANSCRIPT IS THE GPT RESPONSE INTO AUDIO, USERSPEECH IS THE SPEECH FROM USER GOING INTO GPT\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst VoiceToText = ({\n  transcript,\n  setTranscript,\n  setUserSpeech\n}) => {\n  _s();\n  const [isListening, setIsListening] = useState(false);\n  // const [audioFile, setAudioFile] = useState(null);\n  // const [isButtonDisabled, setIsButtonDisabled] = useState(false);\n  const [finalSpeech, setFinalSpeech] = useState(null);\n\n  // useEffect(() => {\n  //     if (transcript) {\n  //       handleGenerateAudio();\n  //     }\n  // }, [transcript]);\n\n  // useEffect(() => {\n  //     if (isButtonDisabled) {\n  //         const timeout = setTimeout(() => {\n  //             setIsButtonDisabled(false);\n  //         }, 2000);\n\n  //         return () => clearTimeout(timeout);\n  //     }\n  // }, [isButtonDisabled]);\n\n  useEffect(() => {\n    console.log('is listening? ' + isListening);\n    if (!isListening && finalSpeech != '') {\n      setUserSpeech(finalSpeech); // puts non-null user speech into the userSpeech variable accessible to gpt\n      setFinalSpeech('');\n    }\n  }, [isListening]);\n  const recognitionRef = useRef(null);\n  const handleStop = () => {\n    setIsListening(false);\n    // setIsButtonDisabled(true);\n    if (recognitionRef.current) {\n      recognitionRef.current.onresult = null;\n      recognitionRef.current.stop();\n    }\n  };\n  const handleStart = () => {\n    setIsListening(true);\n    if (window.SpeechRecognition || window.webkitSpeechRecognition) {\n      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n      if (!recognitionRef.current) {\n        recognitionRef.current = new SpeechRecognition();\n        recognitionRef.current.continuous = true;\n        recognitionRef.current.interimResults = true;\n      }\n      recognitionRef.current.onstart = () => {};\n\n      // automatically on api maybe you can stop the break up when pause\n      // tricks to async waiting for state to update\n      var speech = '';\n      recognitionRef.current.onresult = event => {\n        for (let i = 0; i < event.results.length; i++) {\n          if (event.results[i].isFinal) {\n            const recentSpeech = event.results[i][0].transcript;\n            if (!speech.includes(recentSpeech)) {\n              speech += recentSpeech;\n            }\n            setFinalSpeech(speech);\n            // console.log(finalSpeech);\n            // console.log(speech);\n          }\n        }\n      };\n\n      recognitionRef.current.onerror = event => {\n        console.error(\"Error occurred in recognition:\", event.error);\n      };\n      recognitionRef.current.start();\n    } else {\n      alert(\"Your browser does not support the Web Speech API. Please try a different browser.\");\n    }\n  };\n\n  // const sendTranscriptToBackend = async (text) => {\n  //     const backendUrl = 'http://127.0.0.1:5000/run_script';\n\n  //     try {\n  //         const response = await fetch(backendUrl, {\n  //             method: 'POST',\n  //             headers: {\n  //                 'Content-Type': 'application/json'\n  //             },\n  //             body: JSON.stringify({ text: text })\n  //         });\n\n  //         if (!response.ok) {\n  //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n  //         }\n\n  //         const responseData = await response.json();\n  //         console.log(responseData);\n  //         setAudioFile(responseData.audio_file);\n\n  //     } catch (error) {\n  //         console.error('Error sending transcript to backend:', error);\n  //     }\n  // };\n\n  // const handleGenerateAudio = async () => {\n  //     sendTranscriptToBackend(transcript);\n  //    // handleStop();\n  // };\n  // const sendTranscriptToBackend = async (text) => {\n  //     const apiUrl = \"https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM\";\n  //     const payload = {\n  //         \"text\": text,\n  //         \"model_id\": \"eleven_monolingual_v1\",\n  //         \"voice_settings\": {\n  //             \"stability\": 0,\n  //             \"similarity_boost\": 0,\n  //             \"style\": 0,\n  //             \"use_speaker_boost\": true\n  //         }\n  //     };\n\n  //     const headers = {\n  //         \"Content-Type\": \"application/json\",\n  //         \"xi-api-key\": \"321547ec48256661cb0b640353bde72c\"\n  //     };\n\n  //     try {\n  //         const response = await fetch(apiUrl, {\n  //             method: 'POST',\n  //             headers: headers,\n  //             body: JSON.stringify(payload)\n  //         });\n\n  //         if (!response.ok) {\n  //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n  //         }\n\n  //         const audioBlob = await response.blob();\n  //         const audioUrl = URL.createObjectURL(audioBlob);\n  //         setAudioFile(audioUrl);\n  //         console.log('Response:', response);\n  //         console.log('Audio Blob:', audioBlob);\n  //         console.log('Audio URL:', audioUrl);\n\n  //     } catch (error) {\n  //         console.error('Error sending transcript to Eleven Labs:', error);\n  //     }\n\n  //     handleStop();\n  // };\n\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [/*#__PURE__*/_jsxDEV(\"button\", {\n      id: \"generate-audio-button\"\n      // class={`voice-to-text-button ${isButtonDisabled ? 'disabled' : ''} ${isListening ? 'green' : 'red'}`}\n      ,\n      class: `voice-to-text-button ${isListening ? 'green' : 'red'}`,\n      onClick: isListening ? handleStop : handleStart\n      // disabled={isButtonDisabled}\n      ,\n      children: /*#__PURE__*/_jsxDEV(\"div\", {\n        class: \"mic-icon\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 171,\n        columnNumber: 17\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 164,\n      columnNumber: 13\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"transcript-display\",\n      children: [/*#__PURE__*/_jsxDEV(\"h2\", {\n        children: \"Transcript:\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 184,\n        columnNumber: 17\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: transcript\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 185,\n        columnNumber: 17\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 183,\n      columnNumber: 13\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 163,\n    columnNumber: 9\n  }, this);\n};\n_s(VoiceToText, \"1CNvmjI1nZJm9fs4fSAnR+8gWrk=\");\n_c = VoiceToText;\nexport default VoiceToText;\nvar _c;\n$RefreshReg$(_c, \"VoiceToText\");","map":{"version":3,"names":["React","useState","useRef","useEffect","jsxDEV","_jsxDEV","VoiceToText","transcript","setTranscript","setUserSpeech","_s","isListening","setIsListening","finalSpeech","setFinalSpeech","console","log","recognitionRef","handleStop","current","onresult","stop","handleStart","window","SpeechRecognition","webkitSpeechRecognition","continuous","interimResults","onstart","speech","event","i","results","length","isFinal","recentSpeech","includes","onerror","error","start","alert","children","id","class","onClick","fileName","_jsxFileName","lineNumber","columnNumber","className","_c","$RefreshReg$"],"sources":["/Users/yashagrawal/Documents/Northwestern/Fall2023/CS338/InterviewPrep/src/components/VoiceToText.jsx"],"sourcesContent":["import React, { useState, useRef, useEffect } from 'react';\nimport './VoiceToText.css';\n\n// FOR NOW - TRANSCRIPT IS THE GPT RESPONSE INTO AUDIO, USERSPEECH IS THE SPEECH FROM USER GOING INTO GPT\n\nconst VoiceToText = ({ transcript, setTranscript, setUserSpeech }) => {\n    const [isListening, setIsListening] = useState(false);\n    // const [audioFile, setAudioFile] = useState(null);\n    // const [isButtonDisabled, setIsButtonDisabled] = useState(false);\n    const [finalSpeech, setFinalSpeech] = useState(null);\n\n    // useEffect(() => {\n    //     if (transcript) {\n    //       handleGenerateAudio();\n    //     }\n    // }, [transcript]);\n\n    // useEffect(() => {\n    //     if (isButtonDisabled) {\n    //         const timeout = setTimeout(() => {\n    //             setIsButtonDisabled(false);\n    //         }, 2000);\n\n    //         return () => clearTimeout(timeout);\n    //     }\n    // }, [isButtonDisabled]);\n\n    useEffect(() => {\n        console.log('is listening? ' + isListening);\n        if(!isListening && finalSpeech != ''){\n            setUserSpeech(finalSpeech); // puts non-null user speech into the userSpeech variable accessible to gpt\n            setFinalSpeech('');\n        }\n    }, [isListening]);\n\n    const recognitionRef = useRef(null); \n\n    const handleStop = () => {\n        setIsListening(false);\n        // setIsButtonDisabled(true);\n        if (recognitionRef.current) {\n            recognitionRef.current.onresult = null;  \n            recognitionRef.current.stop();\n        }\n    };\n\n    const handleStart = () => {\n        setIsListening(true);\n        \n        if (window.SpeechRecognition || window.webkitSpeechRecognition) {\n            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n            if (!recognitionRef.current) {\n                recognitionRef.current = new SpeechRecognition();\n                recognitionRef.current.continuous = true;\n                recognitionRef.current.interimResults = true;\n            }\n\n            recognitionRef.current.onstart = () => {};\n\n            // automatically on api maybe you can stop the break up when pause\n            // tricks to async waiting for state to update\n            var speech = '';\n            recognitionRef.current.onresult = (event) => {\n                for (let i = 0; i < event.results.length; i++) {\n                    if (event.results[i].isFinal) {\n                        const recentSpeech = event.results[i][0].transcript;\n                        if(!speech.includes(recentSpeech)){\n                            speech += recentSpeech;\n                        }\n                        setFinalSpeech(speech);\n                        // console.log(finalSpeech);\n                        // console.log(speech);\n                    }\n                }\n            };\n\n            recognitionRef.current.onerror = (event) => {\n                console.error(\"Error occurred in recognition:\", event.error);\n            };\n\n            recognitionRef.current.start();\n        } else {\n            alert(\"Your browser does not support the Web Speech API. Please try a different browser.\");\n        }\n\n    };\n\n    // const sendTranscriptToBackend = async (text) => {\n    //     const backendUrl = 'http://127.0.0.1:5000/run_script';\n\n    //     try {\n    //         const response = await fetch(backendUrl, {\n    //             method: 'POST',\n    //             headers: {\n    //                 'Content-Type': 'application/json'\n    //             },\n    //             body: JSON.stringify({ text: text })\n    //         });\n\n    //         if (!response.ok) {\n    //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n    //         }\n\n    //         const responseData = await response.json();\n    //         console.log(responseData);\n    //         setAudioFile(responseData.audio_file);\n\n    //     } catch (error) {\n    //         console.error('Error sending transcript to backend:', error);\n    //     }\n    // };\n    \n       \n\n    // const handleGenerateAudio = async () => {\n    //     sendTranscriptToBackend(transcript);\n    //    // handleStop();\n    // };\n    // const sendTranscriptToBackend = async (text) => {\n    //     const apiUrl = \"https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM\";\n    //     const payload = {\n    //         \"text\": text,\n    //         \"model_id\": \"eleven_monolingual_v1\",\n    //         \"voice_settings\": {\n    //             \"stability\": 0,\n    //             \"similarity_boost\": 0,\n    //             \"style\": 0,\n    //             \"use_speaker_boost\": true\n    //         }\n    //     };\n\n    //     const headers = {\n    //         \"Content-Type\": \"application/json\",\n    //         \"xi-api-key\": \"321547ec48256661cb0b640353bde72c\"\n    //     };\n\n    //     try {\n    //         const response = await fetch(apiUrl, {\n    //             method: 'POST',\n    //             headers: headers,\n    //             body: JSON.stringify(payload)\n    //         });\n\n    //         if (!response.ok) {\n    //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n    //         }\n\n    //         const audioBlob = await response.blob();\n    //         const audioUrl = URL.createObjectURL(audioBlob);\n    //         setAudioFile(audioUrl);\n    //         console.log('Response:', response);\n    //         console.log('Audio Blob:', audioBlob);\n    //         console.log('Audio URL:', audioUrl);\n\n    //     } catch (error) {\n    //         console.error('Error sending transcript to Eleven Labs:', error);\n    //     }\n\n    //     handleStop();\n    // };\n\n    return (\n        <div>\n            <button\n                id=\"generate-audio-button\"\n                // class={`voice-to-text-button ${isButtonDisabled ? 'disabled' : ''} ${isListening ? 'green' : 'red'}`}\n                class={`voice-to-text-button ${isListening ? 'green' : 'red'}`}\n                onClick={isListening ? handleStop : handleStart}\n                // disabled={isButtonDisabled}\n            >\n                <div class=\"mic-icon\"></div>\n            </button>\n    \n            {/* {audioFile && (\n                <div>\n                    <h3>Your Generated Audio File:</h3>\n                    <audio controls src={audioFile}>\n                        Your browser does not support the audio tag.\n                    </audio>\n                </div>\n            )} */}\n\n            <div className=\"transcript-display\">\n                <h2>Transcript:</h2>\n                <p>{transcript}</p>\n            </div>\n        </div>\n    );\n    \n};\n\nexport default VoiceToText;\n"],"mappings":";;AAAA,OAAOA,KAAK,IAAIC,QAAQ,EAAEC,MAAM,EAAEC,SAAS,QAAQ,OAAO;AAC1D,OAAO,mBAAmB;;AAE1B;AAAA,SAAAC,MAAA,IAAAC,OAAA;AAEA,MAAMC,WAAW,GAAGA,CAAC;EAAEC,UAAU;EAAEC,aAAa;EAAEC;AAAc,CAAC,KAAK;EAAAC,EAAA;EAClE,MAAM,CAACC,WAAW,EAAEC,cAAc,CAAC,GAAGX,QAAQ,CAAC,KAAK,CAAC;EACrD;EACA;EACA,MAAM,CAACY,WAAW,EAAEC,cAAc,CAAC,GAAGb,QAAQ,CAAC,IAAI,CAAC;;EAEpD;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;;EAEAE,SAAS,CAAC,MAAM;IACZY,OAAO,CAACC,GAAG,CAAC,gBAAgB,GAAGL,WAAW,CAAC;IAC3C,IAAG,CAACA,WAAW,IAAIE,WAAW,IAAI,EAAE,EAAC;MACjCJ,aAAa,CAACI,WAAW,CAAC,CAAC,CAAC;MAC5BC,cAAc,CAAC,EAAE,CAAC;IACtB;EACJ,CAAC,EAAE,CAACH,WAAW,CAAC,CAAC;EAEjB,MAAMM,cAAc,GAAGf,MAAM,CAAC,IAAI,CAAC;EAEnC,MAAMgB,UAAU,GAAGA,CAAA,KAAM;IACrBN,cAAc,CAAC,KAAK,CAAC;IACrB;IACA,IAAIK,cAAc,CAACE,OAAO,EAAE;MACxBF,cAAc,CAACE,OAAO,CAACC,QAAQ,GAAG,IAAI;MACtCH,cAAc,CAACE,OAAO,CAACE,IAAI,CAAC,CAAC;IACjC;EACJ,CAAC;EAED,MAAMC,WAAW,GAAGA,CAAA,KAAM;IACtBV,cAAc,CAAC,IAAI,CAAC;IAEpB,IAAIW,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB,EAAE;MAC5D,MAAMD,iBAAiB,GAAGD,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB;MACpF,IAAI,CAACR,cAAc,CAACE,OAAO,EAAE;QACzBF,cAAc,CAACE,OAAO,GAAG,IAAIK,iBAAiB,CAAC,CAAC;QAChDP,cAAc,CAACE,OAAO,CAACO,UAAU,GAAG,IAAI;QACxCT,cAAc,CAACE,OAAO,CAACQ,cAAc,GAAG,IAAI;MAChD;MAEAV,cAAc,CAACE,OAAO,CAACS,OAAO,GAAG,MAAM,CAAC,CAAC;;MAEzC;MACA;MACA,IAAIC,MAAM,GAAG,EAAE;MACfZ,cAAc,CAACE,OAAO,CAACC,QAAQ,GAAIU,KAAK,IAAK;QACzC,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGD,KAAK,CAACE,OAAO,CAACC,MAAM,EAAEF,CAAC,EAAE,EAAE;UAC3C,IAAID,KAAK,CAACE,OAAO,CAACD,CAAC,CAAC,CAACG,OAAO,EAAE;YAC1B,MAAMC,YAAY,GAAGL,KAAK,CAACE,OAAO,CAACD,CAAC,CAAC,CAAC,CAAC,CAAC,CAACxB,UAAU;YACnD,IAAG,CAACsB,MAAM,CAACO,QAAQ,CAACD,YAAY,CAAC,EAAC;cAC9BN,MAAM,IAAIM,YAAY;YAC1B;YACArB,cAAc,CAACe,MAAM,CAAC;YACtB;YACA;UACJ;QACJ;MACJ,CAAC;;MAEDZ,cAAc,CAACE,OAAO,CAACkB,OAAO,GAAIP,KAAK,IAAK;QACxCf,OAAO,CAACuB,KAAK,CAAC,gCAAgC,EAAER,KAAK,CAACQ,KAAK,CAAC;MAChE,CAAC;MAEDrB,cAAc,CAACE,OAAO,CAACoB,KAAK,CAAC,CAAC;IAClC,CAAC,MAAM;MACHC,KAAK,CAAC,mFAAmF,CAAC;IAC9F;EAEJ,CAAC;;EAED;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;EACA;EACA;;EAIA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;;EAEA,oBACInC,OAAA;IAAAoC,QAAA,gBACIpC,OAAA;MACIqC,EAAE,EAAC;MACH;MAAA;MACAC,KAAK,EAAG,wBAAuBhC,WAAW,GAAG,OAAO,GAAG,KAAM,EAAE;MAC/DiC,OAAO,EAAEjC,WAAW,GAAGO,UAAU,GAAGI;MACpC;MAAA;MAAAmB,QAAA,eAEApC,OAAA;QAAKsC,KAAK,EAAC;MAAU;QAAAE,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAM;IAAC;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACxB,CAAC,eAWT3C,OAAA;MAAK4C,SAAS,EAAC,oBAAoB;MAAAR,QAAA,gBAC/BpC,OAAA;QAAAoC,QAAA,EAAI;MAAW;QAAAI,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC,eACpB3C,OAAA;QAAAoC,QAAA,EAAIlC;MAAU;QAAAsC,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAClB,CAAC;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACL,CAAC;AAGd,CAAC;AAACtC,EAAA,CAxLIJ,WAAW;AAAA4C,EAAA,GAAX5C,WAAW;AA0LjB,eAAeA,WAAW;AAAC,IAAA4C,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}