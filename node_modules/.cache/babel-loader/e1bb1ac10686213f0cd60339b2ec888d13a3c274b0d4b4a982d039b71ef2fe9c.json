{"ast":null,"code":"var _jsxFileName = \"/Users/yashagrawal/Documents/Northwestern/Fall2023/CS338/InterviewPrep/src/components/VoiceToText.jsx\",\n  _s = $RefreshSig$();\nimport React, { useState, useRef, useEffect } from 'react';\nimport './VoiceToText.css';\n\n// FOR NOW - TRANSCRIPT IS THE GPT RESPONSE INTO AUDIO, USERSPEECH IS THE SPEECH FROM USER GOING INTO GPT\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst VoiceToText = ({\n  transcript,\n  setTranscript,\n  setUserSpeech\n}) => {\n  _s();\n  const [isListening, setIsListening] = useState(false);\n  const [audioFile, setAudioFile] = useState(null);\n  const [isButtonDisabled, setIsButtonDisabled] = useState(false);\n  const [finalSpeech, setFinalSpeech] = useState(null);\n\n  // useEffect(() => {\n  //     if (transcript) {\n  //       handleGenerateAudio();\n  //     }\n  // }, [transcript]);\n\n  useEffect(() => {\n    if (isButtonDisabled) {\n      const timeout = setTimeout(() => {\n        setIsButtonDisabled(false);\n      }, 2000);\n      return () => clearTimeout(timeout);\n    }\n  }, [isButtonDisabled]);\n  useEffect(() => {\n    console.log('is listening? ' + isListening);\n    if (!isListening && finalSpeech != '') {\n      setUserSpeech(finalSpeech); // puts non-null user speech into the userSpeech variable accessible to gpt\n      setFinalSpeech('');\n    }\n  }, [isListening]);\n  const recognitionRef = useRef(null);\n  const handleStop = () => {\n    setIsListening(false);\n    setIsButtonDisabled(true);\n    if (recognitionRef.current) {\n      recognitionRef.current.onresult = null;\n      recognitionRef.current.stop();\n    }\n  };\n  const handleStart = () => {\n    let finalSpeech = '';\n    if (!isListening) {\n      if (window.SpeechRecognition || window.webkitSpeechRecognition) {\n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        if (!recognitionRef.current) {\n          recognitionRef.current = new SpeechRecognition();\n          recognitionRef.current.continuous = true;\n          recognitionRef.current.interimResults = true;\n        }\n        setIsListening(true);\n        recognitionRef.current.onstart = () => {};\n\n        // automatically on api maybe you can stop the break up when pause\n        // tricks to async waiting for state to update\n        var speech = '';\n        recognitionRef.current.onresult = event => {\n          for (let i = 0; i < event.results.length; i++) {\n            if (event.results[i].isFinal) {\n              const recentSpeech = event.results[i][0].transcript;\n              if (!speech.includes(recentSpeech)) {\n                speech += recentSpeech;\n              }\n              setFinalSpeech(speech);\n              // console.log(finalSpeech);\n              // console.log(speech);\n            }\n          }\n        };\n\n        recognitionRef.current.onerror = event => {\n          console.error(\"Error occurred in recognition:\", event.error);\n        };\n        recognitionRef.current.start();\n      } else {\n        alert(\"Your browser does not support the Web Speech API. Please try a different browser.\");\n      }\n    }\n  };\n\n  // const sendTranscriptToBackend = async (text) => {\n  //     const backendUrl = 'http://127.0.0.1:5000/run_script';\n\n  //     try {\n  //         const response = await fetch(backendUrl, {\n  //             method: 'POST',\n  //             headers: {\n  //                 'Content-Type': 'application/json'\n  //             },\n  //             body: JSON.stringify({ text: text })\n  //         });\n\n  //         if (!response.ok) {\n  //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n  //         }\n\n  //         const responseData = await response.json();\n  //         console.log(responseData);\n  //         setAudioFile(responseData.audio_file);\n\n  //     } catch (error) {\n  //         console.error('Error sending transcript to backend:', error);\n  //     }\n  // };\n\n  // const handleGenerateAudio = async () => {\n  //     sendTranscriptToBackend(transcript);\n  //    // handleStop();\n  // };\n  // const sendTranscriptToBackend = async (text) => {\n  //     const apiUrl = \"https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM\";\n  //     const payload = {\n  //         \"text\": text,\n  //         \"model_id\": \"eleven_monolingual_v1\",\n  //         \"voice_settings\": {\n  //             \"stability\": 0,\n  //             \"similarity_boost\": 0,\n  //             \"style\": 0,\n  //             \"use_speaker_boost\": true\n  //         }\n  //     };\n\n  //     const headers = {\n  //         \"Content-Type\": \"application/json\",\n  //         \"xi-api-key\": \"321547ec48256661cb0b640353bde72c\"\n  //     };\n\n  //     try {\n  //         const response = await fetch(apiUrl, {\n  //             method: 'POST',\n  //             headers: headers,\n  //             body: JSON.stringify(payload)\n  //         });\n\n  //         if (!response.ok) {\n  //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n  //         }\n\n  //         const audioBlob = await response.blob();\n  //         const audioUrl = URL.createObjectURL(audioBlob);\n  //         setAudioFile(audioUrl);\n  //         console.log('Response:', response);\n  //         console.log('Audio Blob:', audioBlob);\n  //         console.log('Audio URL:', audioUrl);\n\n  //     } catch (error) {\n  //         console.error('Error sending transcript to Eleven Labs:', error);\n  //     }\n\n  //     handleStop();\n  // };\n\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [/*#__PURE__*/_jsxDEV(\"button\", {\n      id: \"generate-audio-button\",\n      class: `voice-to-text-button ${isButtonDisabled ? 'disabled' : ''} ${isListening ? 'green' : 'red'}`,\n      onClick: isListening ? handleStop : handleStart,\n      disabled: isButtonDisabled,\n      children: /*#__PURE__*/_jsxDEV(\"div\", {\n        class: \"mic-icon\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 170,\n        columnNumber: 17\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 165,\n      columnNumber: 13\n    }, this), audioFile && /*#__PURE__*/_jsxDEV(\"div\", {\n      children: [/*#__PURE__*/_jsxDEV(\"h3\", {\n        children: \"Your Generated Audio File:\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 175,\n        columnNumber: 21\n      }, this), /*#__PURE__*/_jsxDEV(\"audio\", {\n        controls: true,\n        src: audioFile,\n        children: \"Your browser does not support the audio tag.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 176,\n        columnNumber: 21\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 174,\n      columnNumber: 17\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"transcript-display\",\n      children: [/*#__PURE__*/_jsxDEV(\"h2\", {\n        children: \"Transcript:\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 183,\n        columnNumber: 17\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: transcript\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 184,\n        columnNumber: 17\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 182,\n      columnNumber: 13\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 164,\n    columnNumber: 9\n  }, this);\n};\n_s(VoiceToText, \"4YoJ3cluN5VWIZ9Y1Madyo/bARE=\");\n_c = VoiceToText;\nexport default VoiceToText;\nvar _c;\n$RefreshReg$(_c, \"VoiceToText\");","map":{"version":3,"names":["React","useState","useRef","useEffect","jsxDEV","_jsxDEV","VoiceToText","transcript","setTranscript","setUserSpeech","_s","isListening","setIsListening","audioFile","setAudioFile","isButtonDisabled","setIsButtonDisabled","finalSpeech","setFinalSpeech","timeout","setTimeout","clearTimeout","console","log","recognitionRef","handleStop","current","onresult","stop","handleStart","window","SpeechRecognition","webkitSpeechRecognition","continuous","interimResults","onstart","speech","event","i","results","length","isFinal","recentSpeech","includes","onerror","error","start","alert","children","id","class","onClick","disabled","fileName","_jsxFileName","lineNumber","columnNumber","controls","src","className","_c","$RefreshReg$"],"sources":["/Users/yashagrawal/Documents/Northwestern/Fall2023/CS338/InterviewPrep/src/components/VoiceToText.jsx"],"sourcesContent":["import React, { useState, useRef, useEffect } from 'react';\nimport './VoiceToText.css';\n\n// FOR NOW - TRANSCRIPT IS THE GPT RESPONSE INTO AUDIO, USERSPEECH IS THE SPEECH FROM USER GOING INTO GPT\n\nconst VoiceToText = ({ transcript, setTranscript, setUserSpeech }) => {\n    const [isListening, setIsListening] = useState(false);\n    const [audioFile, setAudioFile] = useState(null);\n    const [isButtonDisabled, setIsButtonDisabled] = useState(false);\n    const [finalSpeech, setFinalSpeech] = useState(null);\n\n    // useEffect(() => {\n    //     if (transcript) {\n    //       handleGenerateAudio();\n    //     }\n    // }, [transcript]);\n\n    useEffect(() => {\n        if (isButtonDisabled) {\n        const timeout = setTimeout(() => {\n            setIsButtonDisabled(false);\n        }, 2000);\n\n        return () => clearTimeout(timeout);\n        }\n    }, [isButtonDisabled]);\n\n    useEffect(() => {\n        console.log('is listening? ' + isListening);\n        if(!isListening && finalSpeech != ''){\n            setUserSpeech(finalSpeech); // puts non-null user speech into the userSpeech variable accessible to gpt\n            setFinalSpeech('');\n        }\n    }, [isListening]);\n\n    const recognitionRef = useRef(null); \n\n    const handleStop = () => {\n        setIsListening(false);\n        setIsButtonDisabled(true);\n        if (recognitionRef.current) {\n            recognitionRef.current.onresult = null;  \n            recognitionRef.current.stop();\n        }\n    };\n\n    const handleStart = () => {\n        let finalSpeech = '';\n        if(!isListening){\n            if (window.SpeechRecognition || window.webkitSpeechRecognition) {\n                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n                if (!recognitionRef.current) {\n                    recognitionRef.current = new SpeechRecognition();\n                    recognitionRef.current.continuous = true;\n                    recognitionRef.current.interimResults = true;\n                }\n\n                setIsListening(true);\n                recognitionRef.current.onstart = () => {};\n\n                // automatically on api maybe you can stop the break up when pause\n                // tricks to async waiting for state to update\n                var speech = '';\n                recognitionRef.current.onresult = (event) => {\n                    for (let i = 0; i < event.results.length; i++) {\n                        if (event.results[i].isFinal) {\n                            const recentSpeech = event.results[i][0].transcript;\n                            if(!speech.includes(recentSpeech)){\n                                speech += recentSpeech;\n                            }\n                            setFinalSpeech(speech);\n                            // console.log(finalSpeech);\n                            // console.log(speech);\n                        }\n                    }\n                };\n\n                recognitionRef.current.onerror = (event) => {\n                    console.error(\"Error occurred in recognition:\", event.error);\n                };\n\n                recognitionRef.current.start();\n            } else {\n                alert(\"Your browser does not support the Web Speech API. Please try a different browser.\");\n            }\n        }\n    };\n\n    // const sendTranscriptToBackend = async (text) => {\n    //     const backendUrl = 'http://127.0.0.1:5000/run_script';\n\n    //     try {\n    //         const response = await fetch(backendUrl, {\n    //             method: 'POST',\n    //             headers: {\n    //                 'Content-Type': 'application/json'\n    //             },\n    //             body: JSON.stringify({ text: text })\n    //         });\n\n    //         if (!response.ok) {\n    //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n    //         }\n\n    //         const responseData = await response.json();\n    //         console.log(responseData);\n    //         setAudioFile(responseData.audio_file);\n\n    //     } catch (error) {\n    //         console.error('Error sending transcript to backend:', error);\n    //     }\n    // };\n    \n       \n\n    // const handleGenerateAudio = async () => {\n    //     sendTranscriptToBackend(transcript);\n    //    // handleStop();\n    // };\n    // const sendTranscriptToBackend = async (text) => {\n    //     const apiUrl = \"https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM\";\n    //     const payload = {\n    //         \"text\": text,\n    //         \"model_id\": \"eleven_monolingual_v1\",\n    //         \"voice_settings\": {\n    //             \"stability\": 0,\n    //             \"similarity_boost\": 0,\n    //             \"style\": 0,\n    //             \"use_speaker_boost\": true\n    //         }\n    //     };\n\n    //     const headers = {\n    //         \"Content-Type\": \"application/json\",\n    //         \"xi-api-key\": \"321547ec48256661cb0b640353bde72c\"\n    //     };\n\n    //     try {\n    //         const response = await fetch(apiUrl, {\n    //             method: 'POST',\n    //             headers: headers,\n    //             body: JSON.stringify(payload)\n    //         });\n\n    //         if (!response.ok) {\n    //             throw new Error(`HTTP error! status: ${response.status} ${response.statusText}`);\n    //         }\n\n    //         const audioBlob = await response.blob();\n    //         const audioUrl = URL.createObjectURL(audioBlob);\n    //         setAudioFile(audioUrl);\n    //         console.log('Response:', response);\n    //         console.log('Audio Blob:', audioBlob);\n    //         console.log('Audio URL:', audioUrl);\n\n    //     } catch (error) {\n    //         console.error('Error sending transcript to Eleven Labs:', error);\n    //     }\n\n    //     handleStop();\n    // };\n\n    return (\n        <div>\n            <button\n                id=\"generate-audio-button\"\n                class={`voice-to-text-button ${isButtonDisabled ? 'disabled' : ''} ${isListening ? 'green' : 'red'}`}\n                onClick={isListening ? handleStop : handleStart}\n                disabled={isButtonDisabled}>\n                <div class=\"mic-icon\"></div>\n            </button>\n    \n            {audioFile && (\n                <div>\n                    <h3>Your Generated Audio File:</h3>\n                    <audio controls src={audioFile}>\n                        Your browser does not support the audio tag.\n                    </audio>\n                </div>\n            )}\n\n            <div className=\"transcript-display\">\n                <h2>Transcript:</h2>\n                <p>{transcript}</p>\n            </div>\n        </div>\n    );\n    \n};\n\nexport default VoiceToText;\n"],"mappings":";;AAAA,OAAOA,KAAK,IAAIC,QAAQ,EAAEC,MAAM,EAAEC,SAAS,QAAQ,OAAO;AAC1D,OAAO,mBAAmB;;AAE1B;AAAA,SAAAC,MAAA,IAAAC,OAAA;AAEA,MAAMC,WAAW,GAAGA,CAAC;EAAEC,UAAU;EAAEC,aAAa;EAAEC;AAAc,CAAC,KAAK;EAAAC,EAAA;EAClE,MAAM,CAACC,WAAW,EAAEC,cAAc,CAAC,GAAGX,QAAQ,CAAC,KAAK,CAAC;EACrD,MAAM,CAACY,SAAS,EAAEC,YAAY,CAAC,GAAGb,QAAQ,CAAC,IAAI,CAAC;EAChD,MAAM,CAACc,gBAAgB,EAAEC,mBAAmB,CAAC,GAAGf,QAAQ,CAAC,KAAK,CAAC;EAC/D,MAAM,CAACgB,WAAW,EAAEC,cAAc,CAAC,GAAGjB,QAAQ,CAAC,IAAI,CAAC;;EAEpD;EACA;EACA;EACA;EACA;;EAEAE,SAAS,CAAC,MAAM;IACZ,IAAIY,gBAAgB,EAAE;MACtB,MAAMI,OAAO,GAAGC,UAAU,CAAC,MAAM;QAC7BJ,mBAAmB,CAAC,KAAK,CAAC;MAC9B,CAAC,EAAE,IAAI,CAAC;MAER,OAAO,MAAMK,YAAY,CAACF,OAAO,CAAC;IAClC;EACJ,CAAC,EAAE,CAACJ,gBAAgB,CAAC,CAAC;EAEtBZ,SAAS,CAAC,MAAM;IACZmB,OAAO,CAACC,GAAG,CAAC,gBAAgB,GAAGZ,WAAW,CAAC;IAC3C,IAAG,CAACA,WAAW,IAAIM,WAAW,IAAI,EAAE,EAAC;MACjCR,aAAa,CAACQ,WAAW,CAAC,CAAC,CAAC;MAC5BC,cAAc,CAAC,EAAE,CAAC;IACtB;EACJ,CAAC,EAAE,CAACP,WAAW,CAAC,CAAC;EAEjB,MAAMa,cAAc,GAAGtB,MAAM,CAAC,IAAI,CAAC;EAEnC,MAAMuB,UAAU,GAAGA,CAAA,KAAM;IACrBb,cAAc,CAAC,KAAK,CAAC;IACrBI,mBAAmB,CAAC,IAAI,CAAC;IACzB,IAAIQ,cAAc,CAACE,OAAO,EAAE;MACxBF,cAAc,CAACE,OAAO,CAACC,QAAQ,GAAG,IAAI;MACtCH,cAAc,CAACE,OAAO,CAACE,IAAI,CAAC,CAAC;IACjC;EACJ,CAAC;EAED,MAAMC,WAAW,GAAGA,CAAA,KAAM;IACtB,IAAIZ,WAAW,GAAG,EAAE;IACpB,IAAG,CAACN,WAAW,EAAC;MACZ,IAAImB,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB,EAAE;QAC5D,MAAMD,iBAAiB,GAAGD,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB;QACpF,IAAI,CAACR,cAAc,CAACE,OAAO,EAAE;UACzBF,cAAc,CAACE,OAAO,GAAG,IAAIK,iBAAiB,CAAC,CAAC;UAChDP,cAAc,CAACE,OAAO,CAACO,UAAU,GAAG,IAAI;UACxCT,cAAc,CAACE,OAAO,CAACQ,cAAc,GAAG,IAAI;QAChD;QAEAtB,cAAc,CAAC,IAAI,CAAC;QACpBY,cAAc,CAACE,OAAO,CAACS,OAAO,GAAG,MAAM,CAAC,CAAC;;QAEzC;QACA;QACA,IAAIC,MAAM,GAAG,EAAE;QACfZ,cAAc,CAACE,OAAO,CAACC,QAAQ,GAAIU,KAAK,IAAK;UACzC,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGD,KAAK,CAACE,OAAO,CAACC,MAAM,EAAEF,CAAC,EAAE,EAAE;YAC3C,IAAID,KAAK,CAACE,OAAO,CAACD,CAAC,CAAC,CAACG,OAAO,EAAE;cAC1B,MAAMC,YAAY,GAAGL,KAAK,CAACE,OAAO,CAACD,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC/B,UAAU;cACnD,IAAG,CAAC6B,MAAM,CAACO,QAAQ,CAACD,YAAY,CAAC,EAAC;gBAC9BN,MAAM,IAAIM,YAAY;cAC1B;cACAxB,cAAc,CAACkB,MAAM,CAAC;cACtB;cACA;YACJ;UACJ;QACJ,CAAC;;QAEDZ,cAAc,CAACE,OAAO,CAACkB,OAAO,GAAIP,KAAK,IAAK;UACxCf,OAAO,CAACuB,KAAK,CAAC,gCAAgC,EAAER,KAAK,CAACQ,KAAK,CAAC;QAChE,CAAC;QAEDrB,cAAc,CAACE,OAAO,CAACoB,KAAK,CAAC,CAAC;MAClC,CAAC,MAAM;QACHC,KAAK,CAAC,mFAAmF,CAAC;MAC9F;IACJ;EACJ,CAAC;;EAED;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;EACA;EACA;;EAIA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;;EAEA,oBACI1C,OAAA;IAAA2C,QAAA,gBACI3C,OAAA;MACI4C,EAAE,EAAC,uBAAuB;MAC1BC,KAAK,EAAG,wBAAuBnC,gBAAgB,GAAG,UAAU,GAAG,EAAG,IAAGJ,WAAW,GAAG,OAAO,GAAG,KAAM,EAAE;MACrGwC,OAAO,EAAExC,WAAW,GAAGc,UAAU,GAAGI,WAAY;MAChDuB,QAAQ,EAAErC,gBAAiB;MAAAiC,QAAA,eAC3B3C,OAAA;QAAK6C,KAAK,EAAC;MAAU;QAAAG,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAM;IAAC;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACxB,CAAC,EAER3C,SAAS,iBACNR,OAAA;MAAA2C,QAAA,gBACI3C,OAAA;QAAA2C,QAAA,EAAI;MAA0B;QAAAK,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC,eACnCnD,OAAA;QAAOoD,QAAQ;QAACC,GAAG,EAAE7C,SAAU;QAAAmC,QAAA,EAAC;MAEhC;QAAAK,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAO,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACP,CACR,eAEDnD,OAAA;MAAKsD,SAAS,EAAC,oBAAoB;MAAAX,QAAA,gBAC/B3C,OAAA;QAAA2C,QAAA,EAAI;MAAW;QAAAK,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC,eACpBnD,OAAA;QAAA2C,QAAA,EAAIzC;MAAU;QAAA8C,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAI,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAClB,CAAC;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACL,CAAC;AAGd,CAAC;AAAC9C,EAAA,CAvLIJ,WAAW;AAAAsD,EAAA,GAAXtD,WAAW;AAyLjB,eAAeA,WAAW;AAAC,IAAAsD,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}